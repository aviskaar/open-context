# PRD: Self-Aware Context Runtime

> **Status**: Draft
> **Author**: OpenContext Team
> **Date**: 2026-02-17
> **Target**: opencontext v0.1.0

---

## 1. Problem Statement

Today, every agentic application (Claude Code, Cursor, Devin, custom Agent SDK apps) starts **cold**. Agents don't know who the user is, what was tried before, what decisions were made, or what the user's preferences are — unless the user manually writes and maintains static instruction files (CLAUDE.md, .cursorrules).

OpenContext already solves part of this: it imports chat history and exposes a context store via MCP. But the store is **passive and dumb** — it saves what agents tell it and returns what agents ask for. It has no understanding of its own contents, no awareness of what's missing, no ability to improve over time.

### What We're Building

A **Self-Aware Context Runtime** — an evolution of the existing OpenContext MCP server and store that:

1. **Lets users define their own context schemas** instead of forcing predefined structures
2. **Maintains a model of itself** — what it knows, what's missing, what's stale, what's contradictory
3. **Observes how agents use it** — tracking reads, writes, misses, and usefulness
4. **Improves autonomously** — suggests schema changes, flags gaps, resolves staleness
5. **Deepens understanding with Ollama** — uses local LLM analysis for semantic contradiction detection, intelligent schema suggestions, context summarization, and smart retrieval — gracefully degrading to deterministic heuristics when Ollama is unavailable

The goal: any agent that connects to OpenContext gets smarter because the context layer itself is smart.

---

## 2. Design Principles

1. **Minimal blast radius** — layer new capabilities onto the existing store, server, and types. Don't rewrite what works.
2. **User-defined over prescriptive** — no hardcoded context types. Users define what matters to them.
3. **Schema-as-instruction** — defining a context type implicitly tells agents what the user cares about tracking.
4. **Self-awareness is queryable** — agents access the system's self-knowledge through the same MCP tool interface they already use.
5. **Additive, not breaking** — existing `save_context`, `recall_context`, etc. keep working unchanged. New capabilities are new tools.
6. **Local-first** — all self-awareness data stays in the local JSON store. No cloud, no network calls.
7. **Graceful degradation** — every Ollama-enhanced feature has a deterministic fallback. The system is fully functional without Ollama; LLM analysis makes it smarter, not operational. This follows the exact pattern already used in `OllamaPreferenceAnalyzer` (`try ollama → catch → generateBasic*`).

---

## 3. Architecture Overview

### Current Architecture (unchanged)

```
Claude / Agent  ←→  MCP (stdio)  ←→  server.ts  ←→  store.ts  ←→  contexts.json
                    REST API      ←→  server.ts  ←→  store.ts  ←→  contexts.json
```

### New Architecture (additive)

```
Claude / Agent  ←→  MCP (stdio)  ←→  server.ts  ←→  store.ts  ←→  contexts.json
                                          │
                                   ┌──────┴──────┐
                                   │  New layers  │
                                   ├──────────────┤
                                   │ schema.ts    │  ← user-defined types
                                   │ awareness.ts │  ← self-model + introspection
                                   │ observer.ts  │  ← usage tracking
                                   │ analyzer.ts  │  ← Ollama-powered deep analysis
                                   └──────────────┘
                                          │
                                   ┌──────┴──────┐
                                   ▼              ▼
                            ~/.opencontext/    Ollama (optional)
                            ├── contexts.json  http://localhost:11434
                            ├── schema.yaml        │
                            └── awareness.json     │ used by analyzer.ts for:
                                                   │ - semantic contradiction detection
                                                   │ - schema suggestions from untyped entries
                                                   │ - context summarization
                                                   │ - smart relevance-ranked retrieval
                                                   │ - stale entry re-evaluation
```

### Files Changed vs Added

| File | Status | What Changes |
|------|--------|-------------|
| `src/mcp/types.ts` | **Modified** | Add `SchemaType`, `SelfModel`, `ObservationEvent`, `AnalysisResult` interfaces |
| `src/mcp/store.ts` | **Modified** | Add schema-aware save/query methods, wrap existing methods with observation hooks |
| `src/mcp/server.ts` | **Modified** | Register 8 new MCP tools alongside existing 11 |
| `src/mcp/schema.ts` | **New** | Schema loader, validator, discovery (~150 lines) |
| `src/mcp/awareness.ts` | **New** | Self-model builder, introspection engine, gap detection (~200 lines) |
| `src/mcp/observer.ts` | **New** | Usage tracking, read/write/miss logging (~100 lines) |
| `src/mcp/analyzer.ts` | **New** | Ollama-powered analysis: contradictions, schema suggestions, summarization, smart retrieval (~250 lines) |
| `src/server.ts` | **Modified** | Add 4 new REST endpoints for schema + awareness + analysis |
| `ui/src/components/SchemaEditor.tsx` | **New** | UI for defining/editing context types |
| `ui/src/components/AwarenessPanel.tsx` | **New** | UI for viewing self-model, gaps, health, analysis results |
| `ui/src/App.tsx` | **Modified** | Add 2 new routes |

**Estimated total new code**: ~850 lines across 4 new files + ~200 lines of modifications to 4 existing files.

---

## 4. Feature 1: User-Defined Context Schemas

### 4.1 Schema Definition File

Users create `~/.opencontext/schema.yaml` (or define schemas through the UI/API). This file declares custom context types with fields, descriptions, and optional constraints.

```yaml
# ~/.opencontext/schema.yaml
version: 1
types:
  decision:
    description: "Architectural or technical decisions with rationale"
    fields:
      what:
        type: string
        required: true
        description: "What was decided"
      why:
        type: string
        required: true
        description: "Reasoning behind the decision"
      alternatives:
        type: string[]
        description: "Options that were considered and rejected"
      project:
        type: string
        description: "Which project this applies to"

  preference:
    description: "User preferences that agents should respect"
    fields:
      domain:
        type: string
        required: true
        description: "Category (code-style, tooling, communication, etc.)"
      rule:
        type: string
        required: true
        description: "The actual preference"
      strength:
        type: enum
        values: [strong, mild, flexible]
        default: mild

  bug_pattern:
    description: "Recurring bugs and their solutions"
    fields:
      symptom:
        type: string
        required: true
      root_cause:
        type: string
      fix:
        type: string
      occurrences:
        type: number
        default: 1
```

- Types are completely freeform — users define what makes sense for them.
- If no `schema.yaml` exists, the system works exactly like today (untyped `ContextEntry` with `content` + `tags`).
- Schemas are optional and additive. Existing entries without a type remain valid.

### 4.2 Implementation: `src/mcp/schema.ts` (new file)

```typescript
// Responsibilities:
// 1. Load and parse schema.yaml from ~/.opencontext/schema.yaml
// 2. Validate entries against their declared type
// 3. Provide schema discovery for agents (list types, describe fields)
// 4. Handle missing/malformed schema gracefully (fall back to untyped)

export interface SchemaField {
  type: 'string' | 'string[]' | 'number' | 'boolean' | 'enum'
  required?: boolean
  description?: string
  values?: string[]     // for enum type
  default?: unknown
}

export interface SchemaType {
  name: string
  description: string
  fields: Record<string, SchemaField>
}

export interface Schema {
  version: number
  types: SchemaType[]
}

export function loadSchema(schemaPath?: string): Schema | null
export function validateEntry(schema: Schema, typeName: string, data: Record<string, unknown>): { valid: boolean; errors: string[] }
export function describeSchema(schema: Schema): string  // human-readable summary for agents
```

### 4.3 Changes to `src/mcp/types.ts`

Add a `contextType` and `structuredData` field to `ContextEntry`:

```typescript
export interface ContextEntry {
  id: string
  content: string                          // existing — stays as the human-readable content
  tags: string[]                           // existing
  source: string                           // existing
  bubbleId?: string                        // existing
  contextType?: string                     // NEW — references a schema type name (e.g. "decision")
  structuredData?: Record<string, unknown> // NEW — typed fields matching the schema
  createdAt: string                        // existing
  updatedAt: string                        // existing
}
```

**Backward compatible**: existing entries without `contextType` or `structuredData` continue to work. The new fields are optional.

### 4.4 Changes to `src/mcp/store.ts`

Add two new methods to the store:

```typescript
// Save a typed context entry — validates against schema if available
saveTypedContext(
  typeName: string,
  data: Record<string, unknown>,
  tags?: string[],
  source?: string,
  bubbleId?: string
): ContextEntry

// Query by type — returns all entries of a given context type
queryByType(
  typeName: string,
  filter?: Record<string, unknown>
): ContextEntry[]
```

The existing `saveContext` method is **unchanged** — it continues to work for untyped entries. `saveTypedContext` is a new method that wraps `saveContext` with schema validation and structured data storage.

### 4.5 New MCP Tools (added to `src/mcp/server.ts`)

| Tool | Arguments | Description |
|------|-----------|-------------|
| `describe_schema` | (none) | Returns all user-defined context types with their fields and descriptions. Agents call this first to understand what context types the user cares about. |
| `save_typed_context` | `type`, `data`, `tags?`, `source?`, `bubbleId?` | Save a context entry with structured data matching a schema type. Falls back to untyped save if type doesn't exist. |
| `query_by_type` | `type`, `filter?` | Query all entries of a given type, optionally filtering by field values. |

### 4.6 New REST Endpoints (added to `src/server.ts`)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/schema` | `GET` | Returns current schema (parsed from schema.yaml) |
| `/api/schema` | `PUT` | Updates schema.yaml (used by UI schema editor) |

### 4.7 UI: Schema Editor Page

New component `ui/src/components/SchemaEditor.tsx` — a form-based editor for adding/editing/removing context types.

- Visual field builder: add fields with name, type, required flag, description
- Live preview of the schema.yaml output
- Template starter schemas (solo-dev, team, open-source-maintainer) available as one-click imports

New route: `/schema` → `SchemaEditor` component (added to `ui/src/App.tsx`).

---

## 5. Feature 2: Self-Awareness (Introspection)

### 5.1 The Self-Model

OpenContext maintains a computed self-model — a compact representation of what it knows about its own state. This is **derived from the store contents**, not separately maintained. It's computed on demand when agents or the UI request it.

```typescript
// src/mcp/awareness.ts

export interface SelfModel {
  identity: {
    owner: string               // from preferences if available
    contextCount: number
    typeBreakdown: Record<string, number>  // entries per type
    bubbleCount: number
    oldestEntry: string         // ISO date
    newestEntry: string         // ISO date
  }

  coverage: {
    typesWithEntries: string[]
    typesEmpty: string[]        // defined in schema but zero entries
    untyped: number             // entries without a contextType
  }

  freshness: {
    recentlyUpdated: number     // entries updated in last 7 days
    stale: number               // entries not updated in 90+ days
    stalestEntries: Array<{ id: string; type?: string; updatedAt: string }>
  }

  gaps: Array<{
    description: string
    severity: 'info' | 'warning'
    suggestion: string
  }>

  contradictions: Array<{
    entryA: string              // id
    entryB: string              // id
    description: string
  }>

  health: {
    coverageScore: number       // 0-1: what % of schema types have entries
    freshnessScore: number      // 0-1: how up-to-date the store is
    overallHealth: 'healthy' | 'needs-attention' | 'sparse'
  }
}
```

### 5.2 How the Self-Model Is Computed

`awareness.ts` exports a single function:

```typescript
export function buildSelfModel(
  store: ReturnType<typeof createStore>,
  schema: Schema | null
): SelfModel
```

It reads from the store and schema to compute:

1. **Identity** — counts, date ranges (direct from `store.listContexts()`)
2. **Coverage** — cross-reference schema types against entries with `contextType` field
3. **Freshness** — compare `updatedAt` timestamps against current date
4. **Gaps** — schema types with zero entries, high-demand types from observer (see Feature 3)
5. **Contradictions** — keyword heuristic as baseline (entries of the same type with opposing terms like "prefer X" vs "avoid X"). When `deep=true` and Ollama is available, upgraded to semantic contradiction detection (see Feature 4, section 7.6)
6. **Health** — computed scores from coverage and freshness metrics

**Default mode requires no LLM.** The baseline self-model is deterministic, fast (<100ms), and always available. The `deep=true` mode enriches it with Ollama-powered analysis when available (see section 7.7).

### 5.3 New MCP Tool: `introspect`

```
Tool: introspect
Arguments: (none)
Returns: Human-readable self-model summary

Example response:
"I am the context store for this workspace.

 I have 47 entries across 3 types:
   - decision: 23 entries (newest: 2 hours ago)
   - preference: 18 entries (newest: 3 days ago)
   - bug_pattern: 6 entries (newest: 2 weeks ago)

 Health: needs-attention
   Coverage: 75% (3 of 4 defined types have entries)
   Freshness: 82% (39 of 47 entries updated within 90 days)

 Gaps:
   ⚠ Type 'deployment_runbook' is defined in your schema but has 0 entries.
   ℹ 8 entries are older than 90 days and may be stale.

 Contradictions:
   ⚠ Entry dec-12 says 'prefer composition over inheritance'
     but entry dec-47 chose class-based inheritance.
     Consider resolving with update_context."
```

This is the **most important new tool**. When an agent calls `introspect` at the start of a session, it immediately understands:
- What context is available and how much to trust it
- Where gaps exist (so it can ask the user directly instead of assuming)
- What contradictions need resolution

### 5.4 New MCP Tool: `get_gaps`

```
Tool: get_gaps
Arguments: (none)
Returns: List of identified gaps with suggestions

Focused subset of introspect — for agents that just want to know
what's missing without the full self-model.
```

### 5.5 New REST Endpoint

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/awareness` | `GET` | Returns the full self-model as JSON (consumed by UI) |

### 5.6 UI: Awareness Panel

New component `ui/src/components/AwarenessPanel.tsx` — a dashboard panel showing the self-model visually.

- Health score with color coding (green/yellow/red)
- Type coverage chart (which types have entries, which are empty)
- Freshness timeline (when entries were last updated)
- Gap list with actionable suggestions
- Contradiction list with links to conflicting entries

New route: `/awareness` → `AwarenessPanel` component.

---

## 6. Feature 3: Usage Observation

### 6.1 What Gets Tracked

Every read and write to the context store is logged as an observation event. This is a **lightweight append-only log** stored in `~/.opencontext/awareness.json`.

```typescript
// src/mcp/observer.ts

export interface ObservationEvent {
  timestamp: string
  action: 'read' | 'write' | 'update' | 'delete' | 'query_miss'
  tool: string                    // which MCP tool or REST endpoint
  contextType?: string            // type queried/written
  entryIds?: string[]             // entries involved
  query?: string                  // what was searched for (on reads/misses)
  agent?: string                  // source identifier if available
}

export interface ObservationLog {
  events: ObservationEvent[]
  summary: {
    totalReads: number
    totalWrites: number
    totalMisses: number           // queries that returned 0 results
    missedQueries: string[]       // unique queries that returned nothing
    typeReadFrequency: Record<string, number>   // how often each type is read
    typeWriteFrequency: Record<string, number>  // how often each type is written
    lastActivity: string          // ISO timestamp
  }
}
```

### 6.2 How Observation Works

The observer wraps existing store methods with minimal instrumentation. In `store.ts`, after each operation, a one-line call logs the event:

```typescript
// In store.ts, existing recallContext method:
recallContext(query: string) {
  const store = load()
  const results = /* existing search logic */

  // NEW: single line addition
  observer.log({ action: results.length > 0 ? 'read' : 'query_miss', query, entryIds: results.map(r => r.id) })

  return results
}
```

This is ~1 line added per existing store method. No logic changes.

### 6.3 Observer Implementation: `src/mcp/observer.ts`

```typescript
// Responsibilities:
// 1. Append events to awareness.json
// 2. Maintain running summary (updated on each append)
// 3. Provide demand signal detection (what queries keep missing)

export function createObserver(observerPath?: string): {
  log(event: Omit<ObservationEvent, 'timestamp'>): void
  getSummary(): ObservationLog['summary']
  getMissedQueries(): string[]           // queries agents asked for but got 0 results
  getTypePopularity(): Record<string, { reads: number; writes: number }>
}
```

### 6.4 How Self-Awareness Uses Observations

The `buildSelfModel` function in `awareness.ts` consumes observer data to enhance gap detection:

```
Schema says type "deployment_runbook" exists but has 0 entries
  → Gap (from schema alone)

Observer says agents searched for "error handling" 5 times with 0 results
  → Gap (from usage observation, even without schema)

Observer says type "decision" is read by agents 10x more than "preference"
  → Insight: decisions are high-value, keep them fresh
```

Observation-powered gaps are **more valuable** than schema-only gaps because they represent **real agent demand**, not just declared structure.

### 6.5 New MCP Tool: `report_usefulness`

```
Tool: report_usefulness
Arguments:
  context_ids: string[]    — which entries were consumed
  useful: boolean          — did the context actually help
  notes?: string           — optional explanation

Returns: Acknowledgment
```

This closes the feedback loop. Agents report back whether the context they retrieved was actually useful. Over time, this builds a signal of which entries are high-value (frequently useful) vs noise (frequently ignored).

### 6.6 Log Rotation

`awareness.json` events are capped at 1000 entries (configurable). When the cap is reached, the oldest 500 events are dropped but their contribution to the running summary is preserved. This keeps the file small while retaining aggregate insights.

---

## 7. Feature 4: Ollama-Powered Deep Analysis

OpenContext already uses Ollama for preference and memory analysis during chat import (see `src/analyzers/ollama-preferences.ts`). This feature extends that same pattern — the `Ollama` SDK, the `try/catch → fallback` degradation, the model/host configuration — into the self-aware runtime.

### 7.1 Design: Always Works Without Ollama

Every Ollama-enhanced capability has a **deterministic baseline** and an **LLM-enhanced mode**:

| Capability | Without Ollama (deterministic) | With Ollama (enhanced) |
|---|---|---|
| Contradiction detection | Keyword heuristic: same-type entries with opposing terms ("prefer X" vs "avoid X") | Semantic analysis: understands that "use composition" and "chose class inheritance" are in tension |
| Schema suggestions | Pattern matching: detect repeated field patterns in untyped `note` entries | Semantic clustering: group untyped entries by meaning, propose named types with descriptions |
| Context summarization | Truncation: first N characters of each entry | Intelligent digest: multi-entry synthesis into a concise briefing |
| Smart retrieval | Substring/AND search (existing `recallContext`/`searchContexts`) | Relevance-ranked: re-rank results by semantic similarity to the query |
| Stale entry evaluation | Timestamp-based: >90 days = stale | Semantic check: "is this entry still relevant given recent entries?" |
| Gap descriptions | Template-based: "Type X has 0 entries" | Natural language: "You track decisions and preferences but have no context about deployment. Recent agent sessions touched deployment 3 times." |

### 7.2 Implementation: `src/mcp/analyzer.ts` (new file)

Reuses the existing Ollama integration patterns from `src/analyzers/ollama-preferences.ts`:

```typescript
import { Ollama } from 'ollama';

export class ContextAnalyzer {
  private ollama: Ollama;
  private model: string;
  private available: boolean | null = null;  // lazy-checked

  constructor(
    model: string = process.env.OLLAMA_MODEL ?? 'gpt-oss:20b',
    host: string = process.env.OLLAMA_HOST ?? 'http://localhost:11434'
  ) {
    this.ollama = new Ollama({ host });
    this.model = model;
  }

  // Check Ollama availability once, cache result
  private async isAvailable(): Promise<boolean> {
    if (this.available !== null) return this.available;
    try {
      const { models } = await this.ollama.list();
      this.available = models.some(m => m.name === this.model);
    } catch {
      this.available = false;
    }
    return this.available;
  }

  // --- Core analysis methods (each with deterministic fallback) ---

  async detectContradictions(entries: ContextEntry[]): Promise<Contradiction[]>
  async suggestSchemaTypes(untypedEntries: ContextEntry[]): Promise<SuggestedType[]>
  async summarizeContext(entries: ContextEntry[], focus?: string): Promise<string>
  async rankByRelevance(entries: ContextEntry[], query: string): Promise<RankedEntry[]>
  async evaluateStaleness(entries: ContextEntry[], recentEntries: ContextEntry[]): Promise<StalenessResult[]>
  async describeGaps(gaps: Gap[], observerSummary: ObservationSummary): Promise<EnrichedGap[]>
}
```

**Key pattern**: every method calls `isAvailable()` first. If Ollama is down, it immediately falls back to the deterministic implementation — no timeout waiting, no retry. Matches existing behavior in `OllamaPreferenceAnalyzer`.

### 7.3 Ollama Prompt Design

Each analysis task uses a focused, structured prompt. Prompts are kept small to work with modest local models.

#### Contradiction Detection Prompt

```
You are analyzing a user's saved context entries for contradictions.

Entries (same type: "preference"):
1. [id: pref-12] "Prefer composition over inheritance in all code"
2. [id: pref-47] "Use class-based repository pattern with inheritance for data layer"

Are these contradictory? If yes, explain the tension in one sentence.
Respond as JSON: { "contradictory": bool, "explanation": string }
```

**Why this works with small models**: binary yes/no question, structured JSON output, minimal context window needed. Each call analyzes a small batch of entries (pairwise comparison within a type), not the entire store.

#### Schema Suggestion Prompt

```
You are analyzing untyped context entries to suggest schema types.

Entries without a type:
1. "Use Redis for caching because Memcached doesn't support data structures"
2. "Chose PostgreSQL over MySQL for JSON column support"
3. "Switched from REST to GraphQL for the mobile API"

These entries seem to follow a pattern. Suggest a schema type name and fields.
Respond as JSON: { "typeName": string, "description": string, "fields": [{ "name": string, "type": "string"|"string[]"|"number", "description": string }] }
```

#### Context Summarization Prompt

```
You are summarizing saved context for an AI agent that is about to start working.

Context entries (type: "decision", project: "my-app"):
1. "Use Drizzle ORM — better SQL control than Prisma"
2. "JWT auth with refresh tokens — stateless for microservices"
3. "Redis pub/sub for real-time features — simpler than WebSockets"

Write a 2-3 sentence briefing an agent can use to understand this project's architecture.
Do not use bullet points. Write in present tense.
```

#### Relevance Ranking Prompt

```
You are ranking context entries by relevance to a query.

Query: "how should I handle authentication?"

Entries:
1. [id: dec-23] "JWT auth with refresh tokens for the API"
2. [id: dec-12] "Use Drizzle ORM for database access"
3. [id: pref-5] "Always prefer stateless approaches"
4. [id: dec-45] "Redis for session caching"

Rank these by relevance (most relevant first).
Respond as JSON: { "ranked": ["dec-23", "pref-5", "dec-45", "dec-12"] }
```

### 7.4 Deterministic Fallbacks (no Ollama)

Each analysis method has an inline fallback:

**Contradiction detection fallback**:
```typescript
// Keyword-based opposition detection
const opposites = [
  ['prefer', 'avoid'], ['use', 'don\'t use'], ['always', 'never'],
  ['composition', 'inheritance'], ['class', 'functional'],
  ['stateful', 'stateless'], ['monolith', 'microservice']
];
// Compare entries of the same type — if entry A contains word X
// and entry B contains its opposite Y, flag as potential contradiction
```

**Schema suggestion fallback**:
```typescript
// Pattern matching on untyped entries
// 1. Group entries by tag overlap
// 2. For each group with 3+ entries, extract common word patterns
// 3. Suggest a type name from the most frequent tag
// 4. Suggest fields from recurring structural patterns
//    (e.g., entries that contain "because" → suggest "reasoning" field)
```

**Summarization fallback**:
```typescript
// Concatenate first 100 chars of each entry, grouped by type
// Return: "N decisions, M preferences, K bug patterns. Most recent: [title]"
```

**Relevance ranking fallback**:
```typescript
// Score by term overlap between query and entry content
// Boost entries whose contextType matches query keywords
// Return sorted by score
```

### 7.5 New MCP Tools (Ollama-enhanced)

| Tool | Arguments | With Ollama | Without Ollama |
|------|-----------|-------------|----------------|
| `analyze_contradictions` | `type?` | Semantic pairwise comparison via LLM | Keyword-opposition heuristic |
| `suggest_schema` | (none) | LLM clusters untyped entries and proposes types | Tag-grouping + pattern matching |
| `summarize_context` | `type?`, `bubbleId?`, `focus?` | LLM-generated briefing paragraph | Truncated concatenation |

These three tools are **in addition to** the 5 tools from Features 1-3, bringing the total new tools to 8.

### 7.6 Integration with Self-Model (`awareness.ts`)

The `buildSelfModel()` function gains an optional `analyzer` parameter:

```typescript
export async function buildSelfModel(
  store: ReturnType<typeof createStore>,
  schema: Schema | null,
  observer?: ReturnType<typeof createObserver>,
  analyzer?: ContextAnalyzer               // NEW — optional
): Promise<SelfModel>
```

When `analyzer` is provided, the self-model is **enriched**:

- **`gaps`** array gets natural-language descriptions instead of template strings
- **`contradictions`** array is populated by semantic analysis instead of (or in addition to) keyword heuristics
- **`health.details`** field gets a human-readable paragraph summarizing the overall state

When `analyzer` is absent or Ollama is down, the self-model is computed deterministically as described in section 5.2. The structure is identical — only the quality of descriptions changes.

### 7.7 Integration with `introspect` Tool

The `introspect` MCP tool gains an optional `deep` argument:

```
Tool: introspect
Arguments:
  deep?: boolean (default: false)

deep=false (default): Deterministic self-model. Fast (<100ms). Always works.
deep=true: Ollama-enhanced self-model. Richer descriptions, semantic
           contradictions, natural-language gap analysis. Slower (2-10s).
           Falls back to deep=false if Ollama unavailable.
```

This lets agents choose: quick introspection at session start (`deep=false`), or thorough analysis when specifically investigating context health (`deep=true`).

### 7.8 Integration with Smart Retrieval

Existing search tools (`recall_context`, `search_contexts`) are **unchanged**. Smart retrieval is exposed as a new behavior on `query_by_type`:

```
Tool: query_by_type
Arguments:
  type: string
  filter?: Record<string, unknown>
  ranked?: boolean (default: false)     // NEW

ranked=false: Returns entries filtered by type/fields (deterministic, fast)
ranked=true: Passes results through Ollama relevance ranking.
             Falls back to ranked=false if Ollama unavailable.
```

This avoids modifying existing tools while making LLM-ranked retrieval opt-in.

### 7.9 New REST Endpoint

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/analyze` | `POST` | Run a specific analysis: `{ action: "contradictions" | "suggest_schema" | "summarize", params: {...} }`. Returns results with `source: "ollama" | "deterministic"` so the UI can indicate which mode was used. |

### 7.10 UI Integration

The `AwarenessPanel.tsx` component (from Feature 2) gains:

- **"Deep Analysis" button** — triggers Ollama-powered introspection, shows richer results when available
- **Ollama status indicator** — green dot when Ollama is reachable and model is loaded, gray when unavailable
- **Analysis source badge** — each insight shows whether it came from deterministic heuristics or LLM analysis
- **Schema suggestion cards** — when `suggest_schema` finds patterns in untyped entries, display proposed types with a one-click "Add to schema" action
- **Contradiction resolution UI** — show conflicting entries side-by-side with the LLM's explanation, let user resolve by editing or deleting

### 7.11 Configuration

Follows the exact same pattern as existing Ollama configuration:

| Setting | Source | Default |
|---------|--------|---------|
| Model | `OLLAMA_MODEL` env var, or `req.body.model`, or UI settings | `gpt-oss:20b` |
| Host | `OLLAMA_HOST` env var, or `req.body.ollamaHost`, or UI settings | `http://localhost:11434` (local), `http://host.docker.internal:11434` (Docker) |
| Enable/disable | `OPENCONTEXT_OLLAMA_ENABLED` env var, or UI toggle | `true` (but gracefully no-ops if unavailable) |

No new configuration surfaces — reuses the existing environment variables and patterns.

### 7.12 Cost and Performance

- **No external API costs** — Ollama runs locally, all inference is free
- **Prompt sizes are small** — each analysis call sends 5-20 entries, not the whole store. Typical prompt is <2000 tokens
- **Pairwise contradiction checks are bounded** — within a type, compare at most `C(n, 2)` pairs. For types with >50 entries, sample the most recent 50
- **Caching** — analysis results for contradiction detection and schema suggestions are cached in `awareness.json` with a TTL (default: 1 hour). Repeated calls within the TTL return cached results
- **Non-blocking** — all Ollama calls are async. The MCP tool returns the deterministic result immediately and can optionally include a `pendingAnalysis: true` flag if the LLM result is still computing (future enhancement)

---

## 8. Process Model: How Continuous Analysis Runs

### 8.1 The Problem

Today, both the HTTP server and MCP server are purely request-response — zero background work, zero `setInterval`, zero scheduled tasks. But the self-aware runtime needs continuous behavior:

- **Observation summaries** need to be periodically rolled up
- **Ollama analysis** (contradictions, schema suggestions) shouldn't block agent requests
- **Stale entry detection** should happen proactively, not only when someone asks
- **Cache invalidation** needs to happen on a schedule

Where does this work live?

### 8.2 Design Decision: No New Daemon

We are **not** introducing a third long-running process. Reasons:

1. **Operational complexity** — users already manage HTTP server + MCP server. A third daemon adds config, monitoring, and failure modes.
2. **Docker complexity** — the single-image model (one CMD) is clean. A background daemon means process supervision (supervisord, tini, etc.).
3. **Overkill for v1** — the volume of background work is small. We don't need a job queue or worker pool.

### 8.3 Approach: Three-Layer Strategy

```
Layer 1: On-demand (immediate, request-driven)
  ↓ most work happens here
Layer 2: Write-triggered (piggyback on mutations)
  ↓ lightweight, automatic
Layer 3: Tick-based (periodic, on the HTTP server)
  ↓ optional, for proactive analysis
```

#### Layer 1: On-Demand Computation (default)

Most analysis is **computed when requested**, not continuously:

```
Agent calls introspect()
  → buildSelfModel() runs synchronously
  → Returns in <100ms (deterministic mode)
  → Cached for 60s (repeated calls within window return cached result)

Agent calls introspect(deep=true)
  → Check cache (TTL: 1 hour for deep analysis)
  → Cache hit: return cached result instantly
  → Cache miss: run Ollama analysis, cache result, return
  → Ollama unavailable: fall back to deterministic, return that
```

This covers the primary use case — agents call `introspect` at session start and get a result. No daemon needed.

**For the MCP server**: this is the **only** layer. The MCP server stays purely request-response. It never does background work. All self-awareness is computed on-demand when agents call tools.

#### Layer 2: Write-Triggered Analysis (piggyback on mutations)

When the store is mutated (save, update, delete), the observer logs the event. On certain mutations, we **piggyback lightweight analysis**:

```typescript
// In store.ts, after saveContext or saveTypedContext:
observer.log({ action: 'write', ... })

// Every N writes (configurable, default: 10), trigger a lightweight re-evaluation:
if (observer.getSummary().totalWrites % 10 === 0) {
  // Async, non-blocking — fire and forget
  awareness.refreshCache(store, schema, observer).catch(() => {})
}
```

`refreshCache` recomputes the deterministic self-model and writes it to `awareness.json`. This means the cached self-model stays reasonably fresh without any background loop — it's refreshed as a side-effect of normal write activity.

**Cost**: ~50ms of extra computation on every 10th write. Imperceptible.

**For Ollama analysis**: write-triggered work does **not** call Ollama. Ollama analysis is only triggered by explicit requests (`deep=true`, `analyze_contradictions`, etc.) or by the optional tick loop (Layer 3). This keeps mutation paths fast.

#### Layer 3: Tick-Based Background Loop (HTTP server only, optional)

The HTTP server gains a **single `setInterval` loop** that runs periodic maintenance. This is the only new long-running behavior in the system.

```typescript
// In src/server.ts, after app.listen():

const TICK_INTERVAL = parseInt(process.env.OPENCONTEXT_TICK_INTERVAL ?? '300000', 10) // 5 minutes default
const ENABLE_BACKGROUND = process.env.OPENCONTEXT_BACKGROUND !== 'false' // opt-out

if (ENABLE_BACKGROUND) {
  setInterval(async () => {
    try {
      await backgroundTick(store, schema, observer, analyzer)
    } catch (error) {
      // Log and continue — background failures are non-fatal
      console.error('[opencontext] background tick failed:', error)
    }
  }, TICK_INTERVAL)
}
```

**What `backgroundTick` does** (sequentially, bounded work per tick):

```typescript
async function backgroundTick(store, schema, observer, analyzer) {
  // 1. Rotate observation log if needed (~0ms if under cap)
  observer.rotateIfNeeded()

  // 2. Refresh deterministic self-model cache (~50ms)
  const selfModel = await buildSelfModel(store, schema, observer)
  cache.set('self-model', selfModel, TTL_1_HOUR)

  // 3. If Ollama available AND deep cache expired, run deep analysis (~5-10s)
  if (analyzer && cache.isExpired('deep-analysis')) {
    const deep = await buildSelfModel(store, schema, observer, analyzer)
    cache.set('deep-analysis', deep, TTL_1_HOUR)
  }

  // 4. Detect new contradictions if store changed since last check
  if (analyzer && store.hasChangedSince(cache.get('last-contradiction-check'))) {
    const contradictions = await analyzer.detectContradictions(store.listContexts())
    cache.set('contradictions', contradictions, TTL_1_HOUR)
  }
}
```

**Key constraints on the tick loop**:

| Constraint | Value | Rationale |
|---|---|---|
| Tick interval | 5 minutes (configurable) | Frequent enough to keep cache warm, infrequent enough to not load the system |
| Max Ollama calls per tick | 2 | Bounded: one for deep self-model, one for contradiction detection |
| Max entries per Ollama call | 50 | Sample recent entries, not full store |
| Total tick duration cap | 30 seconds | If tick exceeds this, skip remaining Ollama work |
| Failure behavior | Log and continue | Background failures never crash the server |
| Disable flag | `OPENCONTEXT_BACKGROUND=false` | Users can opt out entirely |

### 8.4 How the Three Layers Interact

```
Agent calls introspect()
  → Check Layer 3 cache (warm from background tick?)
  → Cache hit: return instantly (<1ms)
  → Cache miss: compute Layer 1 (on-demand, <100ms)
  → Return result

Agent calls introspect(deep=true)
  → Check Layer 3 cache (deep analysis from background tick?)
  → Cache hit: return instantly
  → Cache miss: run Ollama now (Layer 1, 2-10s)
  → Return result

Store mutation (save_typed_context, etc.)
  → Layer 2: log observation, maybe refresh self-model cache
  → Layer 3: next tick will pick up new data automatically

No agents connected, server idle
  → Layer 3 tick still runs every 5 minutes
  → Keeps caches warm so the NEXT agent gets instant results
  → If Ollama is available, deep analysis stays fresh
```

### 8.5 MCP Server vs HTTP Server Responsibilities

| Behavior | MCP Server | HTTP Server |
|---|---|---|
| On-demand computation (Layer 1) | Yes | Yes (via REST API) |
| Write-triggered refresh (Layer 2) | Yes | Yes |
| Background tick loop (Layer 3) | **No** | **Yes** |
| Ollama analysis | On explicit request only | Background + on request |
| Cache reads | Reads from `awareness.json` | Reads from in-memory + `awareness.json` |
| Cache writes | Writes to `awareness.json` | Writes to in-memory + `awareness.json` |

The MCP server stays simple and stateless. The HTTP server gains one `setInterval`. Both benefit from cached results in `awareness.json` — the HTTP server writes them, the MCP server reads them.

**File-based cache sharing**: when the background tick on the HTTP server refreshes the self-model, it writes the result to `awareness.json`. When the MCP server (a separate process) calls `introspect`, it reads from `awareness.json` and finds a warm cache — even though the MCP server never ran any background work itself. This is the same file-sharing pattern already used for `contexts.json`.

### 8.6 Docker Implications

No changes to the Docker model:

```dockerfile
# HTTP server (default CMD) — gets background tick
CMD ["node", "dist/server.js"]

# MCP server (override CMD) — no background tick, reads cached results
# docker run -i ... node dist/mcp/index.js
```

Single image, single process per container, no supervisor needed. The background tick is just a `setInterval` inside the existing HTTP server process.

If running both services (`docker compose up app mcp`), the HTTP server's background tick keeps `awareness.json` fresh, and the MCP server reads it. Coordination is through the filesystem — same pattern as today with `contexts.json`.

### 8.7 Graceful Shutdown

The HTTP server needs to clean up the tick interval on shutdown:

```typescript
let tickInterval: NodeJS.Timeout | null = null

// On startup:
tickInterval = setInterval(backgroundTick, TICK_INTERVAL)

// On shutdown:
process.on('SIGTERM', () => {
  if (tickInterval) clearInterval(tickInterval)
  // Allow in-flight Ollama calls to complete (up to 5s timeout)
  server.close()
})
```

### 8.8 What This Means for Implementation

**Changes to existing files**:

| File | Change |
|---|---|
| `src/server.ts` | Add `setInterval` after `app.listen()`, add `SIGTERM` handler. ~30 lines. |

**New code**:

| Location | What |
|---|---|
| `src/mcp/awareness.ts` | Add `refreshCache()` function and file-based caching logic. ~40 lines. |
| `src/mcp/observer.ts` | Add `rotateIfNeeded()` method. ~10 lines. |

**No new files**. No new processes. No new npm dependencies.

### 8.9 Configuration Summary

| Setting | Default | Description |
|---------|---------|-------------|
| `OPENCONTEXT_BACKGROUND` | `true` | Enable/disable background tick on HTTP server |
| `OPENCONTEXT_TICK_INTERVAL` | `300000` (5 min) | Milliseconds between background ticks |
| `OPENCONTEXT_TICK_TIMEOUT` | `30000` (30s) | Max duration per tick before skipping remaining work |
| `OPENCONTEXT_CACHE_TTL` | `3600000` (1 hr) | How long Ollama analysis results stay cached |

All optional. Zero config needed for the default behavior.

---

## 9. Putting It Together: Agent Interaction Flow

### Session Start (Agent calls introspect)

```
Agent                          OpenContext                    Ollama
  │                                │                            │
  │  describe_schema()             │                            │
  │  ─────────────────────────►    │                            │
  │                                │  Load schema.yaml          │
  │  ◄─────────────────────────    │  Return: 3 types           │
  │                                │                            │
  │  introspect()                  │                            │
  │  ─────────────────────────►    │                            │
  │                                │  buildSelfModel()          │
  │                                │  (deterministic: instant)  │
  │  ◄─────────────────────────    │  Return: health, 2 gaps,   │
  │                                │  1 keyword contradiction   │
  │                                │                            │
  │  introspect(deep=true)         │                            │
  │  ─────────────────────────►    │                            │
  │                                │  buildSelfModel() ────────►│
  │                                │                            │ semantic analysis
  │                                │                  ◄─────────│ richer descriptions
  │  ◄─────────────────────────    │  Return: enriched model,   │
  │                                │  semantic contradictions,  │
  │                                │  natural-language gaps     │
  │                                │                            │
  │  (If Ollama unavailable,       │                            │
  │   deep=true silently falls     │                            │
  │   back to deterministic)       │                            │
```

### During Session (Agent reads, writes, and uses smart retrieval)

```
Agent                          OpenContext                    Ollama
  │                                │                            │
  │  query_by_type("decision",     │                            │
  │    { project: "my-app" },      │                            │
  │    ranked: true)               │                            │
  │  ─────────────────────────►    │                            │
  │                                │  Filter by type ──────────►│
  │                                │                            │ rank by relevance
  │                                │                  ◄─────────│
  │  ◄─────────────────────────    │  Return: 5 decisions       │
  │                                │  (most relevant first)     │
  │                                │                            │
  │  save_typed_context("decision",│                            │
  │    { what: "Use Redis",        │                            │
  │      why: "Need pub/sub...",   │                            │
  │      alternatives: ["RabbitMQ"]│                            │
  │    })                          │                            │
  │  ─────────────────────────►    │                            │
  │                                │  Validate, save, log       │
  │  ◄─────────────────────────    │  Return: saved dec-48      │
  │                                │                            │
  │  summarize_context(            │                            │
  │    type: "decision",           │                            │
  │    focus: "infrastructure")    │                            │
  │  ─────────────────────────►    │                            │
  │                                │  Gather entries ──────────►│
  │                                │                            │ synthesize briefing
  │                                │                  ◄─────────│
  │  ◄─────────────────────────    │  "This project uses        │
  │                                │   Drizzle ORM, JWT auth,   │
  │                                │   and Redis pub/sub..."    │
```

### Session End (Agent reports back)

```
Agent                          OpenContext
  │                                │
  │  report_usefulness(            │
  │    ids: ["dec-12", "dec-23"],   │
  │    useful: true,               │
  │    notes: "ORM decision saved  │
  │     me from recommending       │
  │     Prisma again"              │
  │  )                             │
  │  ─────────────────────────►    │
  │                                │  Log usefulness, update summary
  │  ◄─────────────────────────    │  Return: acknowledged
```

### Periodic Maintenance (User or agent triggers analysis)

```
Agent / UI                     OpenContext                    Ollama
  │                                │                            │
  │  analyze_contradictions()      │                            │
  │  ─────────────────────────►    │                            │
  │                                │  Group entries by type     │
  │                                │  Pairwise compare ────────►│
  │                                │                            │ semantic check
  │                                │                  ◄─────────│
  │  ◄─────────────────────────    │  Return: 2 contradictions  │
  │                                │  with explanations         │
  │                                │                            │
  │  suggest_schema()              │                            │
  │  ─────────────────────────►    │                            │
  │                                │  Gather untyped entries    │
  │                                │  Cluster by meaning ──────►│
  │                                │                            │ propose types
  │                                │                  ◄─────────│
  │  ◄─────────────────────────    │  Return: suggested type    │
  │                                │  "api_decision" with       │
  │                                │  fields: endpoint, method, │
  │                                │  reasoning                 │
```

---

## 10. Storage Schema Evolution

### Current: `~/.opencontext/contexts.json`

```json
{
  "version": 1,
  "entries": [ /* ContextEntry[] */ ],
  "bubbles": [ /* Bubble[] */ ]
}
```

### After: `~/.opencontext/contexts.json` (version bump to 2)

```json
{
  "version": 2,
  "entries": [
    {
      "id": "abc-123",
      "content": "Use Drizzle ORM over Prisma for better SQL control",
      "tags": ["architecture", "database"],
      "source": "claude-code",
      "bubbleId": "bubble-1",
      "contextType": "decision",
      "structuredData": {
        "what": "Use Drizzle ORM over Prisma",
        "why": "Better SQL control, lighter bundle",
        "alternatives": ["Prisma", "Kysely"]
      },
      "createdAt": "2026-02-17T10:00:00Z",
      "updatedAt": "2026-02-17T10:00:00Z"
    },
    {
      "id": "def-456",
      "content": "Prefer functional components over class components",
      "tags": ["code-style"],
      "source": "manual",
      "createdAt": "2026-01-15T08:00:00Z",
      "updatedAt": "2026-01-15T08:00:00Z"
    }
  ],
  "bubbles": [ /* unchanged */ ]
}
```

**Migration**: `store.ts` already has a migration path in `load()` (adds `bubbles` if missing). Add a v1→v2 migration that simply bumps the version number — existing entries without `contextType`/`structuredData` are valid v2 entries (those fields are optional).

### New: `~/.opencontext/schema.yaml`

User-created. See section 4.1 for format.

### New: `~/.opencontext/awareness.json`

```json
{
  "events": [ /* ObservationEvent[] — capped at 1000 */ ],
  "summary": {
    "totalReads": 234,
    "totalWrites": 67,
    "totalMisses": 12,
    "missedQueries": ["error handling", "deployment", "CI config"],
    "typeReadFrequency": { "decision": 89, "preference": 45, "bug_pattern": 12 },
    "typeWriteFrequency": { "decision": 34, "preference": 20, "bug_pattern": 8 },
    "lastActivity": "2026-02-17T14:30:00Z"
  },
  "usefulness": {
    "helpful": { "dec-12": 5, "pref-3": 3 },
    "unhelpful": { "dec-7": 2 }
  }
}
```

---

## 11. Implementation Plan

### Phase 1: User-Defined Schemas (Foundation)

**Goal**: Users can define context types, agents can discover and use them.

| Step | File | Change | Effort |
|------|------|--------|--------|
| 1a | `src/mcp/schema.ts` | New file: schema loader, validator, describer | S |
| 1b | `src/mcp/types.ts` | Add `contextType`, `structuredData` to `ContextEntry` | XS |
| 1c | `src/mcp/store.ts` | Add `saveTypedContext()`, `queryByType()` methods | S |
| 1d | `src/mcp/server.ts` | Register `describe_schema`, `save_typed_context`, `query_by_type` tools | S |
| 1e | `src/server.ts` | Add `GET/PUT /api/schema` endpoints | XS |
| 1f | `ui/src/components/SchemaEditor.tsx` | Schema editor UI page | M |
| 1g | `ui/src/App.tsx` | Add `/schema` route | XS |

**Tests**: Schema loading, validation (valid + invalid entries), migration from v1→v2 store.

### Phase 2: Self-Awareness (Introspection)

**Goal**: OpenContext can describe itself — health, gaps, coverage, contradictions.

| Step | File | Change | Effort |
|------|------|--------|--------|
| 2a | `src/mcp/awareness.ts` | New file: `buildSelfModel()` function | M |
| 2b | `src/mcp/server.ts` | Register `introspect` and `get_gaps` tools | S |
| 2c | `src/server.ts` | Add `GET /api/awareness` endpoint | XS |
| 2d | `ui/src/components/AwarenessPanel.tsx` | Awareness dashboard UI | M |
| 2e | `ui/src/App.tsx` | Add `/awareness` route | XS |

**Tests**: Self-model computation with various store states (empty, sparse, healthy, contradictory).

### Phase 3: Usage Observation (Feedback Loop)

**Goal**: OpenContext tracks how agents use it and feeds insights back into self-awareness.

| Step | File | Change | Effort |
|------|------|--------|--------|
| 3a | `src/mcp/observer.ts` | New file: event logger, summary builder | S |
| 3b | `src/mcp/store.ts` | Add observer hooks to existing methods (~1 line each) | XS |
| 3c | `src/mcp/server.ts` | Register `report_usefulness` tool | XS |
| 3d | `src/mcp/awareness.ts` | Enhance `buildSelfModel()` with observer data | S |

**Tests**: Event logging, log rotation, missed query detection, usefulness tracking.

### Phase 4: Ollama-Powered Deep Analysis

**Goal**: Semantic contradiction detection, intelligent schema suggestions, context summarization, and relevance-ranked retrieval — all with graceful degradation.

| Step | File | Change | Effort |
|------|------|--------|--------|
| 4a | `src/mcp/analyzer.ts` | New file: `ContextAnalyzer` class with 6 analysis methods + deterministic fallbacks | M |
| 4b | `src/mcp/server.ts` | Register `analyze_contradictions`, `suggest_schema`, `summarize_context` tools | S |
| 4c | `src/mcp/server.ts` | Add `deep` arg to `introspect`, `ranked` arg to `query_by_type` | XS |
| 4d | `src/mcp/awareness.ts` | Wire optional `ContextAnalyzer` into `buildSelfModel()` for enriched gaps/contradictions | S |
| 4e | `src/server.ts` | Add `POST /api/analyze` endpoint | XS |
| 4f | `ui/src/components/AwarenessPanel.tsx` | Add deep analysis button, Ollama status indicator, schema suggestion cards, contradiction resolution UI | M |

**Tests**: Each analysis method tested in both modes (Ollama available vs unavailable). Mock Ollama responses for deterministic test behavior. Verify fallback produces valid output for all methods.

### Phase 5: Background Tick Loop (Continuous Analysis)

**Goal**: HTTP server proactively keeps caches warm, runs periodic Ollama analysis, and rotates observation logs — so agents get near-instant results.

| Step | File | Change | Effort |
|------|------|--------|--------|
| 5a | `src/mcp/awareness.ts` | Add `refreshCache()` function and file-based caching with TTL | S |
| 5b | `src/mcp/observer.ts` | Add `rotateIfNeeded()` method | XS |
| 5c | `src/server.ts` | Add `setInterval` tick loop after `app.listen()`, wire `backgroundTick()` function | S |
| 5d | `src/server.ts` | Add `SIGTERM`/`SIGINT` graceful shutdown handler (clear interval, drain in-flight calls) | XS |

**Tests**: Background tick with mocked store (verify cache refresh, rotation). Verify MCP server reads warm cache from `awareness.json` written by HTTP server tick. Verify graceful shutdown clears interval.

### Phase Summary

| Phase | New Files | Modified Files | Estimated New Lines | Dependencies |
|-------|-----------|----------------|-------------------|--------------|
| 1 | 2 (`schema.ts`, `SchemaEditor.tsx`) | 4 (`types.ts`, `store.ts`, `server.ts`, `App.tsx`) + `server.ts` (REST) | ~400 | None (uses existing `js-yaml` or raw YAML parsing) |
| 2 | 2 (`awareness.ts`, `AwarenessPanel.tsx`) | 2 (`server.ts` MCP, `server.ts` REST, `App.tsx`) | ~350 | Phase 1 (schema needed for coverage analysis) |
| 3 | 1 (`observer.ts`) | 3 (`store.ts`, `server.ts`, `awareness.ts`) | ~200 | Phase 2 (awareness consumes observer data) |
| 4 | 1 (`analyzer.ts`) | 4 (`server.ts` MCP, `server.ts` REST, `awareness.ts`, `AwarenessPanel.tsx`) | ~350 | Phases 1-3 + existing `ollama` npm package (already a dependency) |
| 5 | 0 | 3 (`server.ts`, `awareness.ts`, `observer.ts`) | ~100 | Phases 1-4 (wires existing components into a background loop) |
| **Total** | **6 new files** | **~6 existing files modified** | **~1400 lines** | **0 new npm dependencies** (reuses existing `ollama` package) |

---

## 12. New MCP Tool Summary

After implementation, the MCP server exposes **19 tools** (11 existing + 8 new):

### Existing (unchanged)

| Tool | Category |
|------|----------|
| `save_context` | Context CRUD |
| `recall_context` | Context CRUD |
| `list_contexts` | Context CRUD |
| `search_contexts` | Context CRUD |
| `update_context` | Context CRUD |
| `delete_context` | Context CRUD |
| `create_bubble` | Bubble CRUD |
| `list_bubbles` | Bubble CRUD |
| `get_bubble` | Bubble CRUD |
| `update_bubble` | Bubble CRUD |
| `delete_bubble` | Bubble CRUD |

### New (Phases 1-3: Schema, Awareness, Observation)

| Tool | Category | Description |
|------|----------|-------------|
| `describe_schema` | Schema | Returns user-defined context types and fields. Agents call this to understand what context the user cares about. |
| `save_typed_context` | Schema | Save a structured entry matching a user-defined type. |
| `query_by_type` | Schema | Query entries by type with optional field-level filtering. Supports `ranked: true` for Ollama relevance sorting. |
| `introspect` | Awareness | Full self-model: health, coverage, freshness, gaps, contradictions. Supports `deep: true` for Ollama-enhanced analysis. |
| `report_usefulness` | Observation | Agent reports whether retrieved context was helpful. |

### New (Phase 4: Ollama-Powered Analysis)

| Tool | Category | With Ollama | Without Ollama |
|------|----------|-------------|----------------|
| `analyze_contradictions` | Analysis | Semantic pairwise contradiction detection with natural-language explanations | Keyword-opposition heuristic with template explanations |
| `suggest_schema` | Analysis | LLM-powered clustering of untyped entries into proposed schema types with descriptions | Tag-grouping + structural pattern matching |
| `summarize_context` | Analysis | LLM-generated briefing paragraph synthesized from multiple entries | Truncated concatenation with entry counts |

---

## 13. New REST API Summary

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/schema` | `GET` | Returns current schema types |
| `/api/schema` | `PUT` | Updates schema definition |
| `/api/awareness` | `GET` | Returns computed self-model |
| `/api/analyze` | `POST` | Run Ollama analysis: contradictions, schema suggestions, or summarization. Returns `source: "ollama" \| "deterministic"` to indicate which mode was used. |

---

## 14. New UI Routes Summary

| Path | Component | Description |
|------|-----------|-------------|
| `/schema` | SchemaEditor | Define and edit custom context types |
| `/awareness` | AwarenessPanel | View system health, gaps, contradictions |

---

## 15. What We're NOT Building (Scope Boundaries)

| Out of Scope | Reason |
|---|---|
| Cloud LLM APIs (OpenAI, Anthropic) for analysis | Ollama only. Local-first, no API keys, no costs, no data leaving the machine. |
| Auto-generated CLAUDE.md / .cursorrules | Strong feature, but separate PRD. This PRD focuses on the runtime, not the output. |
| Cross-device sync | Requires cloud infrastructure. Keep local-first for v1. |
| SDK npm package (`@opencontext/sdk`) | Separate distribution concern. MCP + REST API is sufficient for v1. |
| Browser extension | Separate product surface. Not needed for the runtime. |
| Team/shared schemas | Multi-user adds auth complexity. Single-user first. |
| Schema marketplace / templates | Community feature. Ship the runtime, templates come later. |
| Autonomous self-improvement actions | v1 surfaces gaps and suggestions. It doesn't auto-act on them. The agent (or user) decides. |
| Streaming Ollama responses | All Ollama calls use `stream: false` for simplicity. Streaming can be added later for long summarizations. |
| Fine-tuning or training on user data | Ollama uses off-the-shelf models. No custom model training. |

---

## 16. Success Criteria

### Functional

- [ ] User can define custom context types in `schema.yaml` and agents discover them via `describe_schema`
- [ ] Agents can save and query typed context entries with field-level filtering
- [ ] `introspect` tool returns accurate health, coverage, freshness, and gap information
- [ ] Existing 11 MCP tools continue to work unchanged (backward compatibility)
- [ ] Store migration from v1 → v2 is automatic and lossless
- [ ] UI allows editing schemas and viewing system awareness
- [ ] `analyze_contradictions` detects semantic tensions between entries when Ollama is available
- [ ] `suggest_schema` proposes meaningful types from untyped entry patterns
- [ ] `summarize_context` produces coherent briefing paragraphs from multiple entries
- [ ] All 3 Ollama-powered tools produce valid, useful output via deterministic fallback when Ollama is unavailable
- [ ] `introspect(deep=true)` enriches gaps and contradictions with natural-language descriptions via Ollama
- [ ] `query_by_type(ranked=true)` returns entries sorted by semantic relevance via Ollama

### Non-Functional

- [ ] `introspect` (default, deterministic) computes in <100ms for stores with up to 1000 entries
- [ ] `introspect(deep=true)` completes in <10s with Ollama (local model)
- [ ] Observer adds <1ms overhead to existing store operations
- [ ] `awareness.json` stays under 200KB with log rotation
- [ ] Zero new npm dependencies required (reuses existing `ollama` package)
- [ ] Each Ollama prompt uses <2000 tokens input to work with modest local models
- [ ] Analysis results are cached in `awareness.json` with 1-hour TTL to avoid redundant Ollama calls
- [ ] Ollama availability check is lazy (first call only) and cached for the session lifetime

---

## 17. Future Directions (Post-v1)

These are explicitly out of scope for v1 but inform the architecture:

1. **Auto-generated instruction files** — produce CLAUDE.md / .cursorrules from the context store + schema, kept in sync automatically
2. **`@opencontext/sdk`** — npm package for agent builders to integrate without MCP, with TypeScript types generated from the user's schema
3. **Schema templates and community packs** — curated starter schemas for common workflows
4. **Session lifecycle** — formal `start_session` / `end_session` tools that let agents declare what they're doing and enable richer handoff between sessions
5. **Hybrid retrieval** — combine Ollama relevance ranking with usefulness scores and read frequency for multi-signal ranking
6. **Cross-agent session handoff** — agent A ends a session, agent B starts one and gets a briefing of what A did (using `summarize_context` under the hood)
7. **Self-improvement actions** — the system not only identifies gaps but proposes concrete prompts to agents: "Next time the user discusses testing, ask them about their E2E preferences and save the answer as a preference entry"
8. **Streaming analysis** — stream Ollama responses for long summarizations to reduce perceived latency
9. **Pluggable LLM backends** — support Ollama, llama.cpp, LM Studio, or cloud APIs through a unified analyzer interface, letting users choose their preferred local inference engine
10. **Automatic schema migration** — when `suggest_schema` proposes a new type and the user accepts, automatically reclassify matching untyped entries into the new type using Ollama
11. **Context embeddings** — generate and cache embeddings for entries using Ollama's embedding models, enabling true vector similarity search alongside keyword search
